# ê²¬ì  AI ê³ ì„±ëŠ¥ ì•„í‚¤í…ì²˜

## ğŸ¯ ì¶”ì²œ ì•„í‚¤í…ì²˜: RAG + Fine-tuning í•˜ì´ë¸Œë¦¬ë“œ

### 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ë²¡í„°í™”

#### Vector Database ì„ íƒ
```yaml
Pinecone (í´ë¼ìš°ë“œ):
  ì¥ì : ì™„ì „ ê´€ë¦¬í˜•, ë¹ ë¥¸ ì†ë„
  ë¹„ìš©: $70/ì›” (ìŠ¤íƒ€í„°)
  ìš©ëŸ‰: 100ë§Œ ë²¡í„°

Qdrant (ì…€í”„í˜¸ìŠ¤íŒ…):
  ì¥ì : ë¬´ë£Œ, ì˜¨í”„ë ˆë¯¸ìŠ¤
  ì„±ëŠ¥: ë§¤ìš° ë¹ ë¦„
  ë©”ëª¨ë¦¬: íš¨ìœ¨ì 

Weaviate (í•˜ì´ë¸Œë¦¬ë“œ):
  ì¥ì : GraphQL ì§€ì›
  íŠ¹ì§•: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
  ìŠ¤ì¼€ì¼: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰
```

#### ì„ë² ë”© ëª¨ë¸
```python
# 1. OpenAI Embeddings (ìµœê³  ì •í™•ë„)
from openai import OpenAI
client = OpenAI()
embedding = client.embeddings.create(
    model="text-embedding-3-large",
    input=ê²¬ì _í…ìŠ¤íŠ¸
)

# 2. í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”©
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embeddings = model.encode(documents)
```

### 2ë‹¨ê³„: LangChain í†µí•©

```python
from langchain import LangChain
from langchain.vectorstores import Pinecone
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# RAG ì²´ì¸ êµ¬ì„±
class EstimateAI:
    def __init__(self):
        self.vectorstore = Pinecone(
            index_name="kis-estimates",
            embedding=OpenAIEmbeddings()
        )
        self.llm = OpenAI(
            model="gpt-4o-mini-finetuned",
            temperature=0.1
        )
        self.chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 5}
            )
        )

    def generate_estimate(self, requirements):
        # 1. ìœ ì‚¬ ê²¬ì  ê²€ìƒ‰
        similar_estimates = self.vectorstore.similarity_search(
            requirements, k=5
        )

        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.build_context(similar_estimates)

        # 3. ê²¬ì  ìƒì„±
        estimate = self.chain.run(
            query=requirements,
            context=context
        )

        return estimate
```

### 3ë‹¨ê³„: Fine-tuning í”„ë¡œì„¸ìŠ¤

#### OpenAI Fine-tuning
```python
# 1. ë°ì´í„° ì¤€ë¹„ (JSONL í˜•ì‹)
{"messages": [
    {"role": "system", "content": "ê²¬ì  ì „ë¬¸ AI"},
    {"role": "user", "content": "ë¶„ì „ë°˜ ì‚¬ì–‘: ..."},
    {"role": "assistant", "content": "ê²¬ì ì„œ: ..."}
]}

# 2. íŒŒì¸íŠœë‹ ì‹¤í–‰
import openai

response = openai.File.create(
    file=open("estimates_training.jsonl", "rb"),
    purpose='fine-tune'
)

fine_tune = openai.FineTuning.create(
    training_file=response.id,
    model="gpt-4o-mini-2024-07-18",
    hyperparameters={
        "n_epochs": 3,
        "batch_size": 1,
        "learning_rate_multiplier": 2
    }
)
```

#### Hugging Face AutoTrain
```bash
# CLIë¡œ ê°„ë‹¨í•˜ê²Œ í›ˆë ¨
autotrain llm \
  --train \
  --model meta-llama/Llama-2-7b-hf \
  --data-path ./estimates_data \
  --text-column text \
  --learning-rate 2e-4 \
  --num-epochs 3 \
  --output-dir ./finetuned_model
```

### 4ë‹¨ê³„: ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ

```python
import redis
import hashlib
import json

class EstimateCache:
    def __init__(self):
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            decode_responses=True
        )
        self.ttl = 86400  # 24ì‹œê°„

    def get_cached_estimate(self, requirements):
        # ìš”êµ¬ì‚¬í•­ í•´ì‹œí™”
        cache_key = hashlib.md5(
            json.dumps(requirements, sort_keys=True).encode()
        ).hexdigest()

        # ìºì‹œ í™•ì¸
        cached = self.redis_client.get(f"estimate:{cache_key}")
        if cached:
            return json.loads(cached)
        return None

    def cache_estimate(self, requirements, estimate):
        cache_key = hashlib.md5(
            json.dumps(requirements, sort_keys=True).encode()
        ).hexdigest()

        self.redis_client.setex(
            f"estimate:{cache_key}",
            self.ttl,
            json.dumps(estimate)
        )
```

### 5ë‹¨ê³„: ì„±ëŠ¥ ìµœì í™”

#### ë³‘ë ¬ ì²˜ë¦¬
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class FastEstimateAI:
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=5)

    async def process_batch(self, requirements_list):
        tasks = []
        for req in requirements_list:
            task = asyncio.create_task(
                self.generate_estimate_async(req)
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks)
        return results

    async def generate_estimate_async(self, requirements):
        # ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì†ë„ 5ë°° í–¥ìƒ
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self.generate_estimate,
            requirements
        )
```

## ğŸ› ï¸ êµ¬í˜„ ë„êµ¬ ë¹„êµ

### RAG ì‹œìŠ¤í…œ êµ¬ì¶• ë„êµ¬

| ë„êµ¬ | ì¥ì  | ë‹¨ì  | ë¹„ìš© | ì¶”ì²œë„ |
|------|------|------|------|--------|
| **LangChain** | í†µí•© ì‰¬ì›€, ë¬¸ì„œ í’ë¶€ | ëŸ¬ë‹ì»¤ë¸Œ | ë¬´ë£Œ | â­â­â­â­â­ |
| **LlamaIndex** | ì„±ëŠ¥ ìš°ìˆ˜, ì»¤ìŠ¤í…€ ê°€ëŠ¥ | ë³µì¡í•¨ | ë¬´ë£Œ | â­â­â­â­ |
| **Haystack** | ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ | ë¬´ê±°ì›€ | ë¬´ë£Œ | â­â­â­ |
| **Semantic Kernel** | MS í†µí•© | ìœˆë„ìš° ì¤‘ì‹¬ | ë¬´ë£Œ | â­â­â­ |

### Fine-tuning í”Œë«í¼

| í”Œë«í¼ | ëª¨ë¸ | ë¹„ìš© | ì†ë„ | ì •í™•ë„ |
|--------|------|------|------|---------|
| **OpenAI** | GPT-4o-mini | $0.008/1K | ë¹ ë¦„ | ìµœê³  |
| **Anthropic** | Claude 3 Haiku | $0.005/1K | ë¹ ë¦„ | ìµœê³  |
| **Hugging Face** | LLaMA 2 | ë¬´ë£Œ(ì…€í”„) | ì¤‘ê°„ | ë†’ìŒ |
| **Google Vertex** | PaLM 2 | $0.002/1K | ë¹ ë¦„ | ë†’ìŒ |
| **Cohere** | Command | $0.003/1K | ë¹ ë¦„ | ë†’ìŒ |

### Vector Database ë¹„êµ

| DB | ì†ë„ | í™•ì¥ì„± | ë¹„ìš© | í•œêµ­ì–´ ì§€ì› |
|----|------|--------|------|-------------|
| **Pinecone** | ìµœê³  | í´ë¼ìš°ë“œ | $70/ì›” | âœ… |
| **Qdrant** | ë§¤ìš°ë¹ ë¦„ | ë¬´ì œí•œ | ë¬´ë£Œ | âœ… |
| **Weaviate** | ë¹ ë¦„ | ëŒ€ê·œëª¨ | ë¬´ë£Œ/$500 | âœ… |
| **ChromaDB** | ë¹ ë¦„ | ì¤‘ê°„ | ë¬´ë£Œ | âœ… |
| **Milvus** | ë¹ ë¦„ | ëŒ€ê·œëª¨ | ë¬´ë£Œ | âœ… |

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ê¸°ì¡´ ì»¤ìŠ¤í…€ AI vs ê°œì„ ëœ AI

| ì§€í‘œ | ê¸°ì¡´ | RAG only | Fine-tuned | RAG+Fine-tuned |
|------|------|----------|------------|----------------|
| **ì •í™•ë„** | 60% | 80% | 85% | 95% |
| **ì†ë„** | 5ì´ˆ | 3ì´ˆ | 2ì´ˆ | 1.5ì´ˆ |
| **ë¹„ìš©/ì›”** | $100 | $150 | $200 | $250 |
| **ë©”ëª¨ë¦¬** | 2GB | 4GB | 2GB | 4GB |
| **í™•ì¥ì„±** | ë‚®ìŒ | ë†’ìŒ | ì¤‘ê°„ | ìµœê³  |

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### 1. í™˜ê²½ ì„¤ì •
```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install langchain pinecone-client openai sentence-transformers

# í™˜ê²½ ë³€ìˆ˜
export OPENAI_API_KEY="sk-..."
export PINECONE_API_KEY="..."
export PINECONE_ENV="us-west1-gcp"
```

### 2. ì´ˆê¸° ë°ì´í„° ì„ë² ë”©
```python
from scripts.embed_estimates import EmbeddingPipeline

pipeline = EmbeddingPipeline()
pipeline.process_historical_estimates("./data/estimates/*.json")
```

### 3. AI ì„œë²„ ì‹¤í–‰
```bash
python app.py --mode hybrid --cache redis --workers 4
```

## ğŸ’° ì˜ˆìƒ ë¹„ìš© (ì›”)

### ì†Œê·œëª¨ (ì¼ 100ê±´)
- OpenAI API: $50
- Vector DB: $0 (Qdrant ì…€í”„í˜¸ìŠ¤íŒ…)
- Fine-tuning: $30 (ì¼íšŒì„±)
- **ì´: $50/ì›”**

### ì¤‘ê·œëª¨ (ì¼ 1000ê±´)
- OpenAI API: $200
- Vector DB: $70 (Pinecone)
- Fine-tuning: $100 (ì¼íšŒì„±)
- **ì´: $270/ì›”**

### ëŒ€ê·œëª¨ (ì¼ 10000ê±´)
- OpenAI API: $1000
- Vector DB: $500 (Pinecone Pro)
- Fine-tuning: $500 (ì¼íšŒì„±)
- ì „ìš© ì„œë²„: $300
- **ì´: $1800/ì›”**

## ğŸ¯ ì¶”ì²œ êµ¬í˜„ ìˆœì„œ

1. **Phase 1**: LangChain + ChromaDB (ë¬´ë£Œ ì‹œì‘)
2. **Phase 2**: OpenAI ì„ë² ë”© ì¶”ê°€
3. **Phase 3**: Pinecone ë§ˆì´ê·¸ë ˆì´ì…˜
4. **Phase 4**: GPT-4o-mini Fine-tuning
5. **Phase 5**: ìºì‹± ë° ìµœì í™”

## ğŸ“ˆ ROI ë¶„ì„

- **êµ¬í˜„ ë¹„ìš©**: ì•½ 500ë§Œì› (2ê°œì›”)
- **ì›” ìš´ì˜ë¹„**: 30ë§Œì›
- **ì˜ˆìƒ íš¨ê³¼**:
  - ê²¬ì  ì‘ì„± ì‹œê°„ 80% ë‹¨ì¶•
  - ì •í™•ë„ 35% í–¥ìƒ
  - ì§ì› ìƒì‚°ì„± 3ë°° ì¦ê°€
- **íˆ¬ì íšŒìˆ˜**: 3-4ê°œì›”